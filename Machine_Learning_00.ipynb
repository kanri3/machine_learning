{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_00.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtk6w7xHy+0syJ2TvmSXj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/machine_learning/blob/main/Machine_Learning_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A54osK8eSdMs"
      },
      "source": [
        "# 機械学習レポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIvZddziTfzk"
      },
      "source": [
        "## 線形回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0hrFij5TrEd"
      },
      "source": [
        "線形回帰モデルは教師あり回帰問題（出力が連続）の学習アルゴリズム  \n",
        "単回帰（説明変数が1個だけ）  \n",
        "重回帰（説明変数が2個以上）  \n",
        "最小二乗法とは？  \n",
        "残差と誤差は違う  \n",
        "残差：回帰直線による推定値である $ax_i + b$ と測定データ $y_i$ との差  \n",
        "残差の二乗の合計が最小になる直線を求めるアルゴリズム  \n",
        "係数：coefficient  \n",
        "切片：intercept  \n",
        "相関係数：相関係数の絶対値が1に近いほど、相関が強い。  \n",
        "回帰係数：回帰直線（学習から得る）の説明変数の係数  \n",
        "分散共分散行列：分散共分散行列の要素である分散および共分散から相関係数・回帰係数を求める。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSwn8xygTAGH"
      },
      "source": [
        "## 非線形回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0jYQ586aqP0"
      },
      "source": [
        "教師あり回帰問題（出力が連続）の学習アルゴリズムである点は線形回帰と同じ。  \n",
        "適用ケース：  \n",
        "訓練データをプロットしたら、明らかに曲線だなーって時  \n",
        "使用する曲線（関数）：  \n",
        "高次関数（2次以上）や指数関数、対数関数等  \n",
        "曲線のチョイスが重要：  \n",
        "散布図から判断。極大値（極小値）の数から、二次関数と三次関数どちらが良いか…。見当違いなことをやってもうまくいかない。  \n",
        "多項式回帰：  \n",
        "非線形回帰の中の一手法と言うことなのだろう。（どこにも明言ないので）  \n",
        "多項式回帰は高次関数（2次以上）でやる。  \n",
        "多項式特徴量：  \n",
        "ある特徴量xに対して、$x^2$ , $x^3$, $x^4$,…を考える。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0mNZUkGTCv3"
      },
      "source": [
        "## ロジスティック回帰モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JNqDMHIYP30"
      },
      "source": [
        "ロジスティック回帰は教師あり分類問題（出力が離散（カテゴリ））の学習アルゴリズム  \n",
        "説明変数がカテゴリカル（質的）な時→線形回帰はムリ  \n",
        "そこで一般化線形モデル  \n",
        "ロジスティック回帰はその中の一つ  \n",
        "最尤法を用いたパラメータ推定  \n",
        "尤度：すべての確率を掛け合わせたもの。  \n",
        "すべての確率って何さ？  \n",
        "ある確率：あるサンプルからモデルで求めた確率。  \n",
        "すべてのサンプルから確率を求めたのがすべての確率  \n",
        "対数尤度関数：  \n",
        "対数による変換は単調な関係が保たれる（同じ$x$で最大に）  \n",
        "対数をとった方が計算が楽に"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APwdMjOOYrRa"
      },
      "source": [
        "## 主成分分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_eLJTGSfUmU"
      },
      "source": [
        "PCA（principal component analysis）  \n",
        "そのアルゴリズムは以下の通り  \n",
        "1）全データの重心を算出（平均値（$x, y, z,$…（または$x_1, x_2, x_3,$…）それぞれの平均））  \n",
        "2）重心からデータの分散（ばらつき）が最大となる方向（第1主成分）を算出  \n",
        "3）第1主成分と直角に交わる（直交）方向で分散が最大となる箇所（第2主成分）を算出  \n",
        "4）直近の主成分と直交する方向で分散が最大となる箇所（第3主成分）を算出  \n",
        "5）4）をデータの次元分だけ繰り返す  \n",
        "\n",
        "より大きい固有値に対応する固有ベクトル（主成分）の方が寄与率が大きい。  \n",
        "\n",
        "主成分は何を意味するのか？解釈　経験，知見に基づくヒューマンな作業  \n",
        "元データの軸（$x, y$（または$x_1, x_2$））を主成分（第1、第2）に変換した散布図を検討"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCjiYF_eYzsR"
      },
      "source": [
        "## アルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhh6BEiKUK4P"
      },
      "source": [
        "###K-近傍法とは？\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNgsA_DgYsdW"
      },
      "source": [
        "kNN(k-Nearest Neighbor method)  \n",
        "前提：既存データが分類済み  \n",
        "そこに新規データをプロットし、その近傍の既存データk個を見つける。（ｋはハイパーパラメータ）  \n",
        "近傍の既存データk個の内の最も多くが属する分類先に新規データを所属させる。  \n",
        "knc : K Neighbors Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaiAiTTKU_Bn"
      },
      "source": [
        "###K-means法とは？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCdpb1WrVEY9"
      },
      "source": [
        "K-平均法  \n",
        "あらかじめいくつのクラスターに分けるかを決め、決めた数の塊にサンプルを分割する方法（非階層クラスター分析）  \n",
        "まず、クラスターの「核」となるk個のサンプルを選ぶ。  \n",
        "次に、各サンプルを最も近い「核」と同じクラスターに分割する。  \n",
        "次に、k個のクラスターの重心点を求め、それを新たな核とする。  \n",
        "重心の位置が変化したなら再びクラスターに分割する。これを重心の位置が変化しなくなるまで繰り返す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibliIvNaSXVu"
      },
      "source": [
        "## サポートベクターマシーン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t88GAaHYbHz"
      },
      "source": [
        "サポートベクターマシーン（SVM）は教師あり分類問題（出力が離散（カテゴリ））の学習アルゴリズム  \n",
        "・要するに何が嬉しいか？  \n",
        "境界が非線形でも解ける。  \n",
        "・（分離）超平面とは？  \n",
        "データを分割する直線（2次元）を常にイメージしてれば良い。  \n",
        "・サポートベクターとは？  \n",
        "アルゴリズムの名前にも出てくるSupport Vectorとは、分離超平面に最も近いデータ点の事である。  \n",
        "・マージンとは？  \n",
        "分離超平面とサポートベクターとの距離。これの最大化こそが当アルゴリズムの目的。  \n",
        "・カーネル法とは？  \n",
        "線形分離不可能な問題だというなら、多次元に拡張しちゃいなよ。  \n",
        "高次元特徴空間にデータを拡張していくと、ある段階で分離超平面により分離可能なデータになるから。"
      ]
    }
  ]
}