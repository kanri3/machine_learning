{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_00.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzlwzll7xciarHxfE71P/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanri3/machine_learning/blob/main/Machine_Learning_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A54osK8eSdMs"
      },
      "source": [
        "# 機械学習レポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIvZddziTfzk"
      },
      "source": [
        "## 線形回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0hrFij5TrEd"
      },
      "source": [
        "線形回帰モデルは教師あり回帰問題（出力が連続）の学習アルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbFj7pk_SBIR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSwn8xygTAGH"
      },
      "source": [
        "## 非線形回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0jYQ586aqP0"
      },
      "source": [
        "教師あり回帰問題（出力が連続）の学習アルゴリズムである点は線形回帰と同じ。  \n",
        "線形回帰（直線）だと相関係数が上がらない時に、  \n",
        "高次関数（2次以上）や指数関数、対数関数等を予め指定し学習を行う。  \n",
        "ぶっちゃけあんま使わない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r_TfFTJTBep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0mNZUkGTCv3"
      },
      "source": [
        "## ロジスティック回帰モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JNqDMHIYP30"
      },
      "source": [
        "ロジスティック回帰は教師あり分類問題（出力が離散（カテゴリ））の学習アルゴリズム\n",
        "\n",
        "feature engineering  \n",
        "特徴抽出  \n",
        "hands on 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APwdMjOOYrRa"
      },
      "source": [
        "## 主成分分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCjiYF_eYzsR"
      },
      "source": [
        "## アルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibliIvNaSXVu"
      },
      "source": [
        "## サポートベクターマシーン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t88GAaHYbHz"
      },
      "source": [
        "サポートベクターマシーン（SVM）は教師あり分類問題（出力が離散（カテゴリ））の学習アルゴリズム  \n",
        "・要するに何が嬉しいか？  \n",
        "境界が非線形でも解ける。  \n",
        "・（分離）超平面とは？  \n",
        "データを分割する直線（2次元）を常にイメージしてれば良い。  \n",
        "・サポートベクターとは？  \n",
        "アルゴリズムの名前にも出てくるSupport Vectorとは、分離超平面に最も近いデータ点の事である。  \n",
        "・マージンとは？  \n",
        "分離超平面とサポートベクターとの距離。これの最大化こそが当アルゴリズムの目的。  \n",
        "・カーネル法とは？  \n",
        "線形分離不可能な問題だというなら、多次元に拡張しちゃいなよ。  \n",
        "高次元特徴空間にデータを拡張していくと、ある段階で分離超平面により分離可能なデータになるから。"
      ]
    }
  ]
}